{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Feature Engineering\n",
    "\n",
    "This notebook performs feature engineering for machine learning models using Spark MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, year, month, dayofmonth, dayofweek, \n",
    "    quarter, when, lit\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, VectorAssembler, StandardScaler, \n",
    "    OneHotEncoder\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/06 11:09:56 WARN Utils: Your hostname, pc-ThinkPad-P15-Gen-1, resolves to a loopback address: 127.0.1.1; using 10.42.101.241 instead (on interface wlp0s20f3)\n",
      "26/01/06 11:09:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/06 11:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FeatureEngineering\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark session initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Amazon Sales: 128,975 rows\n",
      "Pricing: 1,330 rows\n",
      "Inventory: 9,271 rows\n"
     ]
    }
   ],
   "source": [
    "amazon_sales = spark.read.parquet(\"../processed/amazon_sales.parquet\")\n",
    "pricing = spark.read.parquet(\"../processed/pricing_may2022.parquet\")\n",
    "inventory = spark.read.parquet(\"../processed/inventory.parquet\")\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Amazon Sales: {amazon_sales.count():,} rows\")\n",
    "print(f\"Pricing: {pricing.count():,} rows\")\n",
    "print(f\"Inventory: {inventory.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-based features created!\n"
     ]
    }
   ],
   "source": [
    "amazon_sales = amazon_sales.filter(col(\"date\").isNotNull())\n",
    "\n",
    "amazon_sales = amazon_sales.withColumn(\"year\", year(col(\"date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n",
    "    .withColumn(\"dayofweek\", dayofweek(col(\"date\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"date\")))\n",
    "\n",
    "amazon_sales = amazon_sales.withColumn(\n",
    "    \"is_weekend\",\n",
    "    when(col(\"dayofweek\").isin([1, 7]), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "amazon_sales = amazon_sales.withColumn(\n",
    "    \"is_holiday_season\",\n",
    "    when(col(\"month\").isin([11, 12]), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Time-based features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product-based features created!\n"
     ]
    }
   ],
   "source": [
    "inventory_features = inventory.groupBy(\"SKU Code\").agg({\n",
    "    \"stock\": \"first\"\n",
    "})\n",
    "\n",
    "amazon_sales = amazon_sales.join(\n",
    "    inventory_features,\n",
    "    amazon_sales[\"SKU\"] == inventory_features[\"SKU Code\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "category_counts = amazon_sales.groupBy(\"Category\").count()\n",
    "category_avg_price = amazon_sales.groupBy(\"Category\").avg(\"amount\")\n",
    "\n",
    "print(\"Product-based features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer-based features created!\n"
     ]
    }
   ],
   "source": [
    "state_order_count = amazon_sales.groupBy(\"ship-state\").count()\n",
    "state_avg_amount = amazon_sales.groupBy(\"ship-state\").avg(\"amount\")\n",
    "\n",
    "city_order_count = amazon_sales.groupBy(\"ship-city\").count()\n",
    "\n",
    "b2b_avg_amount = amazon_sales.groupBy(\"b2b\").avg(\"amount\")\n",
    "\n",
    "print(\"Customer-based features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pricing-based features created!\n"
     ]
    }
   ],
   "source": [
    "amazon_sales = amazon_sales.withColumn(\n",
    "    \"discount_pct\",\n",
    "    when(\n",
    "        (col(\"amount\") > 0) & (col(\"amount\").isNotNull()) & (col(\"qty\") > 0),\n",
    "        ((col(\"qty\") * 500) - col(\"amount\")) / (col(\"qty\") * 500) * 100\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Pricing-based features created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns to encode: ['Category', 'Size', 'Status', 'ship-state']\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = [\"Category\", \"Size\", \"Status\", \"ship-state\"]\n",
    "\n",
    "indexers = []\n",
    "for col_name in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name,\n",
    "        outputCol=col_name + \"_idx\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    indexers.append(indexer)\n",
    "\n",
    "print(\"Categorical columns to encode:\", categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Feature Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical feature assembler created!\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = [\n",
    "    \"year\", \"month\", \"day\", \"dayofweek\", \"quarter\",\n",
    "    \"is_weekend\", \"is_holiday_season\", \"b2b\", \"qty\"\n",
    "]\n",
    "\n",
    "numeric_assembler = VectorAssembler(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCol=\"numerical_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numerical_features\",\n",
    "    outputCol=\"scaled_numerical_features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "print(\"Numerical feature assembler created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Feature Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature assembler created!\n",
      "Total features: 5\n"
     ]
    }
   ],
   "source": [
    "final_feature_cols = [\"scaled_numerical_features\"] + \\\n",
    "    [col + \"_idx\" for col in categorical_cols]\n",
    "\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=final_feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "print(\"Final feature assembler created!\")\n",
    "print(f\"Total features: {len(final_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering pipeline created!\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers + [numeric_assembler, scaler, final_assembler])\n",
    "\n",
    "print(\"Feature engineering pipeline created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed!\n",
      "Transformed dataset: 128,975 rows\n",
      "Features vector dimension: 13\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(amazon_sales)\n",
    "featured_df = model.transform(amazon_sales)\n",
    "\n",
    "print(\"Feature engineering completed!\")\n",
    "print(f\"Transformed dataset: {featured_df.count():,} rows\")\n",
    "print(f\"Features vector dimension: {len(featured_df.select('features').first()['features'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature-Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-engineered data saved to processed/sales_features.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "featured_df.write.parquet(\n",
    "    \"processed/sales_new_features.parquet\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print(\"Feature-engineered data saved to processed/sales_features.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped!\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
